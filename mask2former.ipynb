{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b5e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from einops import rearrange\n",
    "from swin import Swin_transformer\n",
    "from transformer import Decoder\n",
    "from detectron2.layers import ShapeSpec\n",
    "from position_encoding import PositionEmbeddingSine\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c0213",
   "metadata": {},
   "source": [
    "## Backbone\n",
    "* Swin Transformer tiny version을 사용하였다.\n",
    "* 코드는 https://github.com/tinnunculus/SwinTransformer/blob/main/swin.ipynb 여기 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25b70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_config = {}\n",
    "backbone_config['backbone_patch_size'] = 4\n",
    "backbone_config['backbone_window_size'] = 8\n",
    "backbone_config['backbone_merge_size'] = 2\n",
    "backbone_config['backbone_model_dim'] = 96\n",
    "backbone_config['backbone_num_layers_in_stage'] = [2, 2, 6, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10389840",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = Swin_transformer(\n",
    "    patch_size = backbone_config[\"backbone_patch_size\"], \n",
    "    window_size = backbone_config[\"backbone_window_size\"], \n",
    "    merge_size = backbone_config[\"backbone_merge_size\"], \n",
    "    model_dim = backbone_config[\"backbone_model_dim\"], \n",
    "    num_layers_in_stage = backbone_config[\"backbone_num_layers_in_stage\"]\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6bac598",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.randn((2, 3, 512, 512)).cuda()\n",
    "features = backbone(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf078291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 96, 128, 128])\n",
      "torch.Size([2, 192, 64, 64])\n",
      "torch.Size([2, 384, 32, 32])\n",
      "torch.Size([2, 768, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(features['res2'].shape)\n",
    "print(features['res3'].shape)\n",
    "print(features['res4'].shape)\n",
    "print(features['res5'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be007c",
   "metadata": {},
   "source": [
    "## Pixel Decoder\n",
    "* Maskformer의 Pixel Decoder와 다르게 Deformable Transformer의 Encoder를 사용하였다.\n",
    "* Deformable Transformer는 https://github.com/facebookresearch/Mask2Former/blob/main/mask2former/modeling/pixel_decoder/msdeformattn.py 에서 가져왔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d97d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Deformable Transformer Encoder는 \n",
    "https://github.com/facebookresearch/Mask2Former/mask2former/modeling/pixel_decoder/msdeformattn.py 에서 가져왔다.\n",
    "'''\n",
    "\n",
    "'''\n",
    "    MSDeformAttnPixelDecoder :\n",
    "    \n",
    "        Args:\n",
    "            input_shape:    \n",
    "                'res2':(channels =  96, height=None, width=None, stride =  4)\n",
    "                'res3':(channels = 192, height=None, width=None, stride =  8)\n",
    "                'res4':(channels = 384, height=None, width=None, stride = 16)\n",
    "                'res5':(channels = 768, height=None, width=None, stride = 32)\n",
    "            transformer_dropout: 0.0\n",
    "            transformer_nheads: 8\n",
    "            transformer_dim_feedforward: 1024\n",
    "            transformer_enc_layers: 6\n",
    "            conv_dims: 256\n",
    "            mask_dim: 256\n",
    "            norm (str or callable): 'GN'\n",
    "            transformer_in_features: ['res3', 'res4', 'res5']\n",
    "            common_stride: 4\n",
    "            \n",
    "        Input:\n",
    "            features : \n",
    "                'res2': torch.Size([1,  96, 328, 200])\n",
    "                'res3': torch.Size([1, 192, 164, 100])\n",
    "                'res4': torch.Size([1, 384,  82,  50])\n",
    "                'res5': torch.Size([1, 768,  41,  25])\n",
    "            \n",
    "        Output:\n",
    "            mask_features : torch tensor [1, 256, res2_h, res2_w],\n",
    "            transformer_encoder_features : torch tensor [1, 256, res5_h, res5_w],\n",
    "            multi_scale_features : [\n",
    "                torch tensor [1, 256, res5_h, res5_w],\n",
    "                torch tensor [1, 256, res4_h, res4_w],\n",
    "                torch tensor [1, 256, res3_h, res3_w]\n",
    "            ]\n",
    "'''\n",
    "\n",
    "from msdeformattn import MSDeformAttnPixelDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54edef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pixel Decoder configuration\n",
    "pixel_decoder_config = {}\n",
    "pixel_decoder_config['input_shape'] = {}\n",
    "pixel_decoder_config['input_shape']['res2'] = ShapeSpec(channels=96, height=None, width=None, stride=4)\n",
    "pixel_decoder_config['input_shape']['res3'] = ShapeSpec(channels=192, height=None, width=None, stride=8)\n",
    "pixel_decoder_config['input_shape']['res4'] = ShapeSpec(channels=384, height=None, width=None, stride=16)\n",
    "pixel_decoder_config['input_shape']['res5'] = ShapeSpec(channels=768, height=None, width=None, stride=32)\n",
    "\n",
    "pixel_decoder_config['transformer_dropout'] = 0.0\n",
    "pixel_decoder_config['transformer_nheads'] = 8\n",
    "pixel_decoder_config['transformer_dim_feedforward'] = 1024\n",
    "pixel_decoder_config['transformer_enc_layers'] = 6\n",
    "pixel_decoder_config['conv_dims'] = 256\n",
    "pixel_decoder_config['mask_dim'] = 256\n",
    "pixel_decoder_config['norm'] = 'GN'\n",
    "pixel_decoder_config['transformer_in_features'] = ['res3', 'res4', 'res5']\n",
    "pixel_decoder_config['common_stride'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9b0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_decoder = MSDeformAttnPixelDecoder(\n",
    "    input_shape = pixel_decoder_config['input_shape'], \n",
    "    transformer_dropout = pixel_decoder_config['transformer_dropout'],\n",
    "    transformer_nheads = pixel_decoder_config['transformer_nheads'],\n",
    "    transformer_dim_feedforward = pixel_decoder_config['transformer_dim_feedforward'],\n",
    "    transformer_enc_layers = pixel_decoder_config['transformer_enc_layers'],\n",
    "    conv_dim = pixel_decoder_config['conv_dims'],\n",
    "    mask_dim = pixel_decoder_config['mask_dim'],\n",
    "    norm = pixel_decoder_config['norm'],\n",
    "    transformer_in_features = pixel_decoder_config['transformer_in_features'],\n",
    "    common_stride = pixel_decoder_config['common_stride'],\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12a9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_features, transformer_encoder_features, multi_scale_features = pixel_decoder.forward_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53773f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 16, 16])\n",
      "torch.Size([2, 256, 16, 16])\n",
      "torch.Size([2, 256, 32, 32])\n",
      "torch.Size([2, 256, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(mask_features.shape)\n",
    "print(transformer_encoder_features.shape)\n",
    "for feature in multi_scale_features:\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd05d1b",
   "metadata": {},
   "source": [
    "## Transformer decoder\n",
    "* 기존의 cross attention을 mask cross attention으로 대체.\n",
    "* self attention 과 cross attention의 위치 변경\n",
    "* Learnable한 query vectors\n",
    "* pixel decoder에서 Transformer decoder로 들어가는 feature map은 linear mapping을 한번 거친다.\n",
    "* nn.MultiheadAttention에서는 N, B, C 순서의 query, key, value type이니 주의해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4805c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Masked_attention(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mh_attention = nn.MultiheadAttention(embed_dim = model_dim, num_heads = num_heads)\n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "        \n",
    "    def forward(self, query, value, key_pos, attn_mask):\n",
    "        key = value + key_pos\n",
    "        \n",
    "        out = self.mh_attention(\n",
    "            query = query,\n",
    "            key = key,\n",
    "            value = value,\n",
    "            attn_mask = attn_mask\n",
    "        )[0]\n",
    "        \n",
    "        return self.norm(out + query)\n",
    "    \n",
    "class Self_attention(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mh_attention = nn.MultiheadAttention(embed_dim = model_dim, num_heads = num_heads)\n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "        \n",
    "    def forward(self, query):        \n",
    "        out = self.mh_attention(\n",
    "            query = query,\n",
    "            key = query,\n",
    "            value = query\n",
    "        )[0]\n",
    "        \n",
    "        return self.norm(out + query)\n",
    "    \n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, model_dim, inter_dim):\n",
    "        super(FFN, self).__init__()\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(model_dim, inter_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(inter_dim, model_dim)\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ffn(x) + x\n",
    "        return self.norm(x)\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, model_dim = 256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "class Transformer_decoder_block(nn.Module):\n",
    "    def __init__(self, model_dim = 256, num_heads = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.masked_attention = Masked_attention(model_dim, num_heads)\n",
    "        self.self_attention = Self_attention(model_dim, num_heads)\n",
    "        self.ffn = FFN(model_dim, 2*model_dim)\n",
    "        \n",
    "    def forward(self, query, value, key_pos, attn_mask):\n",
    "        query = self.masked_attention(query, value, key_pos, attn_mask)\n",
    "        out = self.self_attention(query)\n",
    "        out = self.ffn(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Transformer_decoder(nn.Module):\n",
    "    def __init__(self, n_class = 10, L = 3, num_query = 100, num_features = 3, model_dim = 256, num_heads = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_heads = num_heads\n",
    "        self.transformer_block = nn.ModuleList([Transformer_decoder_block(model_dim=model_dim, num_heads=num_heads) for _ in range(L * 3)])\n",
    "        self.query = nn.Parameter(torch.rand(num_query, 1, model_dim))\n",
    "        \n",
    "        self.from_features_linear = nn.ModuleList([nn.Conv2d(model_dim, model_dim, kernel_size=1) for _ in range(num_features)])\n",
    "        self.from_features_bias = nn.ModuleList([nn.Embedding(1, model_dim) for _ in range(num_features)])\n",
    "        self.pos_emb = PositionEmbeddingSine(model_dim // 2, normalize=True)\n",
    "        \n",
    "        self.decoder_norm = nn.LayerNorm(model_dim)\n",
    "        self.classfication_module = nn.Linear(model_dim, n_class)\n",
    "        self.segmentation_module = MLP(model_dim)\n",
    "        \n",
    "    def forward_prediction_heads(self, mask_embed, pix_emb, decoder_layer_size=None):\n",
    "        mask_embed = self.decoder_norm(mask_embed)\n",
    "        mask_embed = mask_embed.transpose(0, 1) # b, 100, 256\n",
    "        outputs_class = self.classfication_module(mask_embed)\n",
    "        mask_embed = self.segmentation_module(mask_embed)\n",
    "        outputs_mask = torch.einsum(\"bqc,bchw->bqhw\", mask_embed, pix_emb)\n",
    "        \n",
    "        if decoder_layer_size is not None:\n",
    "            attn_mask = F.interpolate(outputs_mask, size=decoder_layer_size, mode=\"bilinear\", align_corners=False)\n",
    "            attn_mask = (attn_mask.sigmoid().flatten(2).unsqueeze(1).repeat(1, self.num_heads, 1, 1).flatten(0, 1) < 0.5).bool() # head 수 만큼 복사한다. bool 형으로 넣어야 한다. True인 곳이 무시할 픽셀\n",
    "            attn_mask = attn_mask.detach()\n",
    "        else:\n",
    "            attn_mask = None\n",
    "        return outputs_class, outputs_mask, attn_mask\n",
    "\n",
    "        \n",
    "    def forward(self, features, pix_emb):\n",
    "        query = self.query.expand(self.query.shape[0], features[0].shape[0], self.query.shape[2]) # batch 만큼 복사\n",
    "        \n",
    "        predictions_class = []\n",
    "        predictions_mask = []\n",
    "        \n",
    "        for i in range(self.num_features):\n",
    "            b, c, h, w = features[i].shape\n",
    "                                \n",
    "            kv = self.from_features_linear[i](features[i])  + self.from_features_bias[i].weight[:, :, None, None]\n",
    "            kv = rearrange(kv, 'b c h w-> (h w) b c')\n",
    "            \n",
    "            key_pos = self.pos_emb(b, h, w, features[i].device, None)\n",
    "            key_pos = rearrange(key_pos, 'b c h w -> (h w) b c')\n",
    "            \n",
    "            for j in range(3):\n",
    "                outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(query, mask_features, decoder_layer_size=(h, w))\n",
    "                # axial training을 위해 중간 결과를 저장한다.\n",
    "                predictions_class.append(outputs_class)\n",
    "                predictions_mask.append(outputs_mask)\n",
    "                \n",
    "                attn_mask[torch.where(attn_mask.sum(-1) == attn_mask.shape[-1])] = False # 중간 추출된 mask가 아무것도 가리키지 않을 경우 global context attention으로 처리한다.\n",
    "                query = self.transformer_block[i * 3 + j](query, kv, key_pos, attn_mask)\n",
    "                \n",
    "        outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(query, mask_features, decoder_layer_size=None)\n",
    "        predictions_class.append(outputs_class)\n",
    "        predictions_mask.append(outputs_mask)\n",
    "                \n",
    "        out = {\n",
    "            'pred_logits': predictions_class[-1],\n",
    "            'pred_masks': predictions_mask[-1],\n",
    "            'aux_outputs': {\n",
    "                'pred_logits' : predictions_class,\n",
    "                'pred_masks': predictions_mask,\n",
    "            }\n",
    "        }\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e90c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_decoder_config = {}\n",
    "transformer_decoder_config['n_class'] = 10\n",
    "transformer_decoder_config['L'] = 3\n",
    "transformer_decoder_config['num_query'] = 100\n",
    "transformer_decoder_config['num_features'] = 3\n",
    "transformer_decoder_config['model_dim'] = 256\n",
    "transformer_decoder_config['num_heads'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ee493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_decoder = Transformer_decoder(\n",
    "    n_class = transformer_decoder_config['n_class'] + 1, \n",
    "    L = transformer_decoder_config['L'], \n",
    "    num_query = transformer_decoder_config['num_query'], \n",
    "    num_features = transformer_decoder_config['num_features'], \n",
    "    model_dim = transformer_decoder_config['model_dim'], \n",
    "    num_heads = transformer_decoder_config['num_heads']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7538b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer_decoder(multi_scale_features, mask_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f6804f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pred_logits', 'pred_masks', 'aux_outputs'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46fd7ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 11])\n",
      "torch.Size([2, 100, 128, 128])\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "print(out['pred_logits'].shape)\n",
    "print(out['pred_masks'].shape)\n",
    "print(len(out['aux_outputs']['pred_logits']), len(out['aux_outputs']['pred_masks']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc36793",
   "metadata": {},
   "source": [
    "## Matching\n",
    "* DETR과 동일하게 bipartite maching을 한다.\n",
    "* MaskFormer와는 다르게 focal loss 대신에 cross entropy를 사용한다.\n",
    "* MaskFormer와는 다르게 모든 픽셀에 대해서 distance를 계산하지 않고 임의의 추출된 픽셀에 대해서만 계산한다.\n",
    "* 포인트를 임의로 추출하는 것은 모든 이미지에 대해서 동일한 위치의 픽셀을 추출한다.\n",
    "* 112 * 112 개의 픽셀을 추출하는데 feature map의 크기가 112, 112 보다 작을 수도 있다. 그렇기에 중복 추출을 허용한다. F.grid_sample 함수를 이용\n",
    "* matching 하는데 있어서는 마지막 단 layer만을 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6685f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    def __init__(self, n_sample = 112 * 112, w_class: float = 1, w_ce: float = 1, w_dice: float = 1):\n",
    "        super().__init__()\n",
    "        self.n_sample = n_sample\n",
    "        self.w_class = w_class\n",
    "        self.w_ce = w_ce\n",
    "        self.w_dice = w_dice\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def dice_cost(self, predict, target):\n",
    "        # predict : b * n_queries, n_sample_points\n",
    "        # target : b * n_obj, n_sample_points\n",
    "        numerator = 2 * (predict[:, None, :] * target[None, :, :]).sum(-1)\n",
    "        denominator = predict.sum(-1)[:, None] + target.sum(-1)[None, :]\n",
    "        cost_dice = 1 - (numerator + 1) / (denominator + 1)\n",
    "        return cost_dice\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def ce_cost(self, predict, target):\n",
    "        # predict : b * n_queries, n_sample_points\n",
    "        # target : b * n_obj, n_sample_points\n",
    "        predict = predict[:, None, :].expand((predict.shape[0], target.shape[0], predict.shape[1]))\n",
    "        target = target[None, :, :].expand((predict.shape[0], target.shape[0], target.shape[1]))\n",
    "        ce = F.binary_cross_entropy_with_logits(predict, target, reduction='none')\n",
    "        \n",
    "        return ce.mean(-1)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, out, targets):\n",
    "        pred_logits = out[\"pred_logits\"] # b, n, class + 1\n",
    "        pred_masks = out[\"pred_masks\"] # b, n, h, w\n",
    "        target_logits = targets[\"labels\"] # [ m_i for i in b]\n",
    "        target_masks = targets[\"masks\"] # [ m_i, h, w for i in b]\n",
    "        bs, num_queries = pred_logits.shape[:2]\n",
    "        device = pred_logits.device\n",
    "        \n",
    "        out_prob = pred_logits.flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        tgt_ids = torch.cat([v for v in target_logits]) # [batch_size * num_obj]\n",
    "        \n",
    "        \n",
    "        out_mask = pred_masks.flatten(0, 1).unsqueeze(1)  # [batch_size * num_queries, 1, h, w]\n",
    "        tgt_mask = torch.cat([v for v in target_masks]).unsqueeze(1) # [batch_size * num_obj, 1, h, w]\n",
    "        grid = torch.rand((1, 1, self.n_sample, 2), device=out_mask.device) * 2 - 1\n",
    "        out_grid = grid.expand(out_mask.shape[0], *grid.shape[1:])\n",
    "        out_mask = F.grid_sample(out_mask, out_grid, mode='nearest', align_corners=False).squeeze()  # [batch_size * num_queries, n_sample_points]\n",
    "        tgt_grid = grid.expand(tgt_mask.shape[0], *grid.shape[1:])\n",
    "        tgt_mask = F.grid_sample(tgt_mask, tgt_grid, mode='nearest' , align_corners=False).squeeze()  # [batch_size * num_obj, n_sample_points]\n",
    "\n",
    "        # cost :\n",
    "        #     row : pred_querys\n",
    "        #     col : target_obj\n",
    "        cost_class = -out_prob[:, tgt_ids]                   # [batch_size * num_queries, batch_size * num_obj]\n",
    "        cost_dice = self.dice_cost(out_mask, tgt_mask)       # [batch_size * num_queries, batch_size * num_obj] \n",
    "        cost_ce = self.ce_cost(out_mask, tgt_mask)           # [batch_size * num_queries, batch_size * num_obj]\n",
    "        \n",
    "        # Final cost matrix\n",
    "        C = self.w_dice * cost_dice + self.w_class * cost_class + self.w_ce * cost_ce\n",
    "        C = C.view(bs, num_queries, -1).cpu() # [batch_size, num_queries, batch_size * num_obj]\n",
    "        \n",
    "        sizes = [len(v) for v in target_masks]\n",
    "        \n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        result = []\n",
    "        for i, j in indices:\n",
    "            i = torch.as_tensor(i, dtype=torch.int64, device=device)\n",
    "            j = torch.as_tensor(j, dtype=torch.int64, device=device)\n",
    "            result.append(i[j])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63b1869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher_config = {}\n",
    "matcher_config['n_sample'] = 112 * 112\n",
    "matcher_config['w_class'] = 1.0\n",
    "matcher_config['w_ce'] = 20.0\n",
    "matcher_config['w_dice'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b814f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher(\n",
    "    n_sample = matcher_config['n_sample'],\n",
    "    w_class = matcher_config['w_class'],\n",
    "    w_ce = matcher_config['w_ce'],\n",
    "    w_dice = matcher_config['w_dice']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73b8a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 예시 정답 데이터\n",
    "target = {}\n",
    "target['labels'] = [torch.zeros((15), dtype=torch.long).cuda(), torch.ones((4), dtype=torch.long).cuda()]\n",
    "target['masks'] = [torch.zeros((15, 128, 128)).cuda(), torch.ones((4, 128, 128)).cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e49100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_indexs = matcher(out, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4201e8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0, 61, 26, 11, 89, 67, 20, 32, 99, 25, 68, 28, 74, 45, 22],\n",
      "       device='cuda:0'), tensor([56, 80, 53,  5], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(match_indexs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ede00",
   "metadata": {},
   "source": [
    "## Loss\n",
    "* 전체적으로 MaskFormer 의 Loss function 과 동일하다.\n",
    "* Mask loss에 focal loss 대신 cross entropy를 사용한다.\n",
    "* 모든 매칭에 대해서 classfication loss를 적용한다.\n",
    "* object가 있는 매칭에 대해서만 mask loss를 적용한다.\n",
    "* macher와 마찬가지로 point sampling을 하는데, Uniform 하게 샘플링 했던 matcher와는 정답 픽셀(foreground)에서 더 많이 뽑도록 한다. 이를 위해서 detectron2의 get_uncertain_point_coords_with_randomness 함수를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "697cf342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.projects.point_rend.point_features import get_uncertain_point_coords_with_randomness\n",
    "\n",
    "class Maskformer_loss(nn.Module):\n",
    "    def __init__(self, n_sample = 112 * 112, w_ce = 1., w_dice = 1., w_class = 1., w_noobj = 1., oversample_ratio = 3.0, importance_sample_ratio = 0.75):\n",
    "        super(Maskformer_loss, self).__init__()\n",
    "        self.n_sample = n_sample\n",
    "        self.w_class = w_class\n",
    "        self.w_ce = w_ce\n",
    "        self.w_dice = w_dice\n",
    "        self.w_noobj = w_noobj\n",
    "        self.oversample_ratio = oversample_ratio\n",
    "        self.importance_sample_ratio = importance_sample_ratio\n",
    "        \n",
    "    def class_loss(self, pred_logits, target_logits, match_indexs):\n",
    "        device = pred_logits.device\n",
    "        target_labels = torch.zeros(pred_logits.shape[:2], dtype=torch.int64, device=device)\n",
    "        cost_no_obj = torch.ones(pred_logits.shape[2], device=device)\n",
    "        cost_no_obj[0] *= self.w_noobj\n",
    "        \n",
    "        for i, match_index in enumerate(match_indexs):\n",
    "            target_labels[i, match_index] = target_logits[i]\n",
    "        \n",
    "        class_loss = F.cross_entropy(pred_logits.flatten(0, 1), target_labels.flatten(0, 1), cost_no_obj)\n",
    "        return class_loss\n",
    "        \n",
    "    def ce_loss(self, predict, target, gamma = 2.0, alpha = 0.25):\n",
    "        # predict : b * n_queries, h * w\n",
    "        # target : b * n_obj, h * w\n",
    "        ce = F.binary_cross_entropy_with_logits(predict, target, reduction='none')\n",
    "\n",
    "        return ce.mean()\n",
    "        \n",
    "    def dice_loss(self, predict, target):\n",
    "        numerator = 2 * (predict * target).sum(-1)\n",
    "        denominator = predict.sum(-1) + target.sum(-1)\n",
    "        loss_dice = 1 - (numerator + 1) / (denominator + 1)\n",
    "        return loss_dice.mean()\n",
    "    \n",
    "    def calculate_uncertainty(self, logits):\n",
    "        assert logits.shape[1] == 1\n",
    "        gt_class_logits = logits.clone()\n",
    "        return -(torch.abs(gt_class_logits))\n",
    "\n",
    "    def forward(self, out, targets, match_indexs):\n",
    "        pred_logits = out[\"pred_logits\"] # b, n, class + 1\n",
    "        pred_masks = out[\"pred_masks\"] # b, n, h, w\n",
    "        target_logits = targets[\"labels\"] # [ m_i for i in b]\n",
    "        target_boxes = targets[\"masks\"] # [ m_i, h, w for i in b]\n",
    "        \n",
    "        tgt_mask = torch.cat([v for v in target_boxes]).unsqueeze(1) # [batch_size * num_obj, 1, h, w]\n",
    "        out_mask = pred_masks  # [batch_size, num_queries, h, w]\n",
    "        out_mask = torch.cat([out_mask[i, match_index, :] for i, match_index in enumerate(match_indexs)]).unsqueeze(1)  # [batch_size * num_obj, 1, h, w]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            point_coords = get_uncertain_point_coords_with_randomness(\n",
    "                out_mask,\n",
    "                lambda logits: self.calculate_uncertainty(logits),\n",
    "                self.n_sample,\n",
    "                self.oversample_ratio,\n",
    "                self.importance_sample_ratio,\n",
    "            ).unsqueeze(1)\n",
    "            \n",
    "            tgt_mask = F.grid_sample(tgt_mask, point_coords, mode='nearest', align_corners=False).squeeze(1) # [batch_size * num_queries, n_sample_points]\n",
    "        out_mask = F.grid_sample(out_mask, point_coords, mode='nearest', align_corners=False).squeeze(1) # [batch_size * num_queries, n_sample_points]\n",
    "\n",
    "        class_loss = self.class_loss(pred_logits, target_logits, match_indexs) * self.w_class\n",
    "        ce_loss = self.ce_loss(out_mask, tgt_mask) * self.w_ce\n",
    "        dice_loss = self.dice_loss(out_mask, tgt_mask) * self.w_dice\n",
    "        \n",
    "        return class_loss + ce_loss + dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1e93d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_config = {}\n",
    "loss_config['n_sample'] = 112 * 112\n",
    "loss_config['w_class'] = 1.0\n",
    "loss_config['w_ce'] = 20.0\n",
    "loss_config['w_dice'] = 1.0\n",
    "loss_config['w_noobj'] = 0.1\n",
    "loss_config['oversample_ratio'] = 3.0\n",
    "loss_config['importance_sample_ratio'] = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0de1de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = Maskformer_loss(\n",
    "    n_sample = loss_config['n_sample'] , \n",
    "    w_ce = loss_config['w_class'] , \n",
    "    w_dice = loss_config['w_ce'] , \n",
    "    w_class = loss_config['w_dice'], \n",
    "    w_noobj = loss_config['w_noobj'], \n",
    "    oversample_ratio = loss_config['oversample_ratio'], \n",
    "    importance_sample_ratio = loss_config['importance_sample_ratio']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9babf62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 19.00613\n"
     ]
    }
   ],
   "source": [
    "loss = Loss(out, target, match_indexs)\n",
    "print('loss: %.5f' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12229304",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
